Core Elements
Symbol	Meaning	â€œEqualsâ€ / Plain English

ð‘ 
s	state	what the world looks like right now

ð‘ 
â€²
s
â€²
	next state	the state after taking an action

ð‘Ž
a	action	what the agent does

ð‘Ž
â€²
a
â€²
	next action	the next action considered or taken

ð‘Ÿ
r	reward	score/feedback from the environment

ð›¾
Î³	discount factor	how much future rewards matter (0â€“1)

ð‘…
R / 
ðº
G	return	total reward over time (maybe discounted)
Policy (how the agent chooses actions)
Symbol	Meaning	Equals / Plain English

ðœ‹
Ï€	policy	the agentâ€™s decision-making rule
**(\pi_\theta(a	s))**	probability of action 
ð‘Ž
a in state 
ð‘ 
s

ðœƒ
Î¸	parameters of policy network	the weights of the neural network
**(\log \pi_\theta(a	s))**	log-probability
Value & Q-Functions
Symbol	Meaning	Equals / Plain English

ð‘‰
ðœƒ
(
ð‘ 
)
V
Î¸
	â€‹

(s)	value function	predicted future reward from state 
ð‘ 
s

ð‘„
ðœƒ
(
ð‘ 
,
ð‘Ž
)
Q
Î¸
	â€‹

(s,a)	Q-value / action-value	predicted future reward from taking action 
ð‘Ž
a in 
ð‘ 
s

max
â¡
ð‘Ž
â€²
ð‘„
ðœƒ
(
ð‘ 
â€²
,
ð‘Ž
â€²
)
max
a
â€²
	â€‹

Q
Î¸
	â€‹

(s
â€²
,a
â€²
)	best next action value	the highest-valued next action

ðœƒ
âˆ’
Î¸
âˆ’
	target network parameters	a stable, slowly updated copy for DQN
Advantage and Advantage-Based Terms
Symbol	Meaning	Equals / Plain English

ð´
(
ð‘ 
,
ð‘Ž
)
A(s,a)	advantage	how much better/worse an action was compared to normal
Positive A	advantage > 0	action was good â†’ probability should go up
Negative A	advantage < 0	action was bad â†’ probability should go down
Loss Function Components
Symbol	Meaning	Equals / Plain English

ð¿
L	loss	what training tries to minimize
policy loss	(-\log\pi(a	s) \cdot A(s,a))
value loss	
(
ð‘‰
(
ð‘ 
)
âˆ’
ð‘…
)
2
(V(s)âˆ’R)
2
	train the value network to predict returns
TD error	
ð‘Ÿ
+
ð›¾
max
â¡
ð‘„
(
ð‘ 
â€²
,
ð‘Ž
â€²
)
âˆ’
ð‘„
(
ð‘ 
,
ð‘Ž
)
r+Î³maxQ(s
â€²
,a
â€²
)âˆ’Q(s,a)	â€œWhat happened vs what you predictedâ€
TD loss	square of TD error	
TD
2
TD
2

entropy 
ð»
(
ðœ‹
)
H(Ï€)	exploration term	encourages randomness / exploration
Returns (Total Reward Calculations)
Symbol	Meaning	Equals / Plain English

ðº
ð‘¡
G
t
	â€‹

	discounted return	all future rewards starting at time t
formula	
ðº
ð‘¡
=
ð‘Ÿ
ð‘¡
+
ð›¾
ð‘Ÿ
ð‘¡
+
1
+
ð›¾
2
ð‘Ÿ
ð‘¡
+
2
+
â€¦
G
t
	â€‹

=r
t
	â€‹

+Î³r
t+1
	â€‹

+Î³
2
r
t+2
	â€‹

+â€¦	future rewards shrink by Î³
ðŸ“Œ Super Simple Version (One Sentence Each)

State 
ð‘ 
s = where you are.

Action 
ð‘Ž
a = what you do.

Reward 
ð‘Ÿ
r = what you get for doing it.

Policy 
ðœ‹
Ï€ = how you decide what to do.

Value 
ð‘‰
V = how good a state is.

Q-value 
ð‘„
Q = how good an action is in a state.

Advantage 
ð´
A = how much better an action was than expected.

Gamma 
ð›¾
Î³ = how much you care about future rewards.

Loss 
ð¿
L = the number the algorithm tries to minimize to improve itself.